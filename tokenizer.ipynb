{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\selas\\Downloads\\taylorswift.txt\", 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "text\n",
    "tokens=text.encode(\"utf-8\")\n",
    "tokens=list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "class BasicTokenizer():\n",
    "    def __init__(self, pattern=None):\n",
    "\n",
    "        self.pattern=GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern=re.compile(self.pattern)\n",
    "\n",
    "     \n",
    "\n",
    "    def get_stats(self, ids, counts=None):\n",
    "        counts={} if counts==None else counts\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "\n",
    "            counts[pair]=counts.get(pair, 0)+1\n",
    "        return counts\n",
    "\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        \n",
    "        new_ids=[]\n",
    "        i=0\n",
    "        while i<len(ids):\n",
    "            if i < len(ids) - 1 and ids[i]==pair[0] and ids[i+1]==pair[1]:\n",
    "                new_ids.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i+=1\n",
    "            #print(f\"merging{pair}----->{idx}\")\n",
    "        return new_ids\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    def train(self, text, vocab_size):\n",
    "        \n",
    "        text_chunks = self.compiled_pattern.findall(text)\n",
    "\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    " \n",
    "            \n",
    "        \n",
    "        num_merges=vocab_size-256\n",
    "        merges={}\n",
    "        vocab={idx: bytes([idx]) for idx in range(256)}\n",
    "        idx=256\n",
    "        for i in range(num_merges):\n",
    "            stats={}\n",
    "            for chunk_ids in ids:\n",
    "\n",
    "                self.get_stats(chunk_ids, stats)\n",
    "\n",
    "\n",
    "            pair=max(stats, key=stats.get)\n",
    "            ids = [self.merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "                #print(stats)\n",
    "                \n",
    "            ids=self.merge(ids, pair, idx)\n",
    "            merges[pair]=idx\n",
    "            '''if i%1000==0:\n",
    "                print(f\"merging({pair}) to {idx})\")'''\n",
    "                #print(len(ids))\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            idx+=1\n",
    "        \n",
    "        self.merges=merges\n",
    "        self.vocab=vocab\n",
    "        #return(merges)\n",
    "        print(f\"compression ratio: {len(text) / len(ids):.2f}X\")\n",
    "\n",
    "         \n",
    "        \n",
    "\n",
    "    def encode_chunk(self, text_bytes):\n",
    "        ids=list(text_bytes)\n",
    "        while len(ids)>=2:\n",
    "            stats=self.get_stats(ids)\n",
    "            #print(self.merges)\n",
    "            pair=min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx=self.merges[pair]\n",
    "            ids=self.merge(ids, pair, idx)\n",
    "        return ids\n",
    "    def encode_ordinary(self, text):\n",
    "        chuncks=self.compiled_pattern.findall(text)\n",
    "        ids=[]\n",
    "        for chunk in chuncks:\n",
    "            chunk_bytes=chunk.encode('utf-8')\n",
    "            chunk_ids=self.encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        tokens=b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text=tokens.decode(\"utf-8\", errors='replace')\n",
    "        return text\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression ratio: 4.01X\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokin=BasicTokenizer()\n",
    "tokin.train(text, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordinary bro\n"
     ]
    }
   ],
   "source": [
    "print(tokin.decode(tokin.encode_ordinary(\"ordinary bro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
